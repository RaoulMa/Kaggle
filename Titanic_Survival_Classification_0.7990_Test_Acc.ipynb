{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "4d0f1227-9788-4cf8-a9cf-21bbfad4db3a",
        "_uuid": "17aaaacd8c9701d4aecc4fcd650770fcc93e2147"
      },
      "cell_type": "markdown",
      "source": "**Author:** Raoul Malm  \n\n**Description:** Given is a training set of samples listing passengers who survived or did not survive the Titanic disaster. The goal is to construct a model that can predict from a test dataset not containing the survival information if these passengers in the test dataset survived or not. This is a supervised classification task. The individual steps for the solution are:\n- Analyze data\n- Manipulate data: complete, convert, create, delete features\n- Model data with kNN, SVC, Decision Tree, Random Forest, Neural Networks\n\n**Outline:**\n1. Libraries and settings\n2. Analyze data\n3. Manipulate data\n4. Model data\n5. Predict and submit test results\n\n**Results:** \n- Using a split of 90%/10% on the labeled training data this implementation, training on data of 801 passengers, achieves a 82% accuracy on the validation set of 90 passengers. Using all data on the test set achieves 79.90% accuracy.\n\n**Reference:** [Titanic Data Science Solutions by Manav Sehgal](https://www.kaggle.com/startupsci/titanic-data-science-solutions?scriptVersionId=1145136)\n\n"
    },
    {
      "metadata": {
        "_cell_guid": "6bacf1cb-9713-450b-9107-60190cc8c767",
        "_uuid": "266d8130e738c87f2149790be96ffab9ca99ce94"
      },
      "cell_type": "markdown",
      "source": "# 1. Libraries and settings"
    },
    {
      "metadata": {
        "_cell_guid": "811372be-a4a3-4b03-a1dd-66a36f96a834",
        "scrolled": true,
        "_uuid": "5cfd2df9914284894f08d8f702efed041897fbac",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport sklearn.linear_model\nimport sklearn.svm\nimport sklearn.ensemble\nimport sklearn.neighbors\nimport sklearn.naive_bayes\nimport sklearn.tree\nimport sklearn.neural_network\nfrom subprocess import check_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\n%matplotlib inline\n\nvalid_set_size_percentage = 10.0; # 10% = default\ntrain_on_all_data = True; # for submission of test results otherwise False\ncv_num = 1; # number of cross validations; = 1 for submission of test results\n\n#display parent directory and working directory\nprint(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\nprint(os.getcwd()+':', os.listdir(os.getcwd()));",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e35a0af9-f2f5-4c9d-b1ab-cd55d34dc368",
        "_uuid": "76c019d3fd8b019a37badd2ca90dd68df746d3aa"
      },
      "cell_type": "markdown",
      "source": "# 2. Analyze data\n\nThe train/test sets have 891/418 rows with 12/11 columns. The features are:\n- Survived: 0 = No, 1 = Yes \n- Pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd \n- Name: Name of the passenger\n- Sex: male, female \n- Age: Age in years. Is fractional if less than 1. If the age is estimated, it is in the form of xx.5.\n- SibSp: # of siblings / spouses aboard the Titanic (Sibling = brother, sister, stepbrother, stepsister, Spouse = husband, wife). Mistresses and fianc√©s were ignored\n- Parch: # of parents / children aboard the Titanic (Parent = mother, father, Child = daughter, son, stepdaughter, stepson). Some children travelled only with a nanny, therefore Parch=0 for them.\n- Ticket: Ticket number \n- Fare: Passenger fare \n- Cabin: Cabin number \n- Embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\nThe features can be characterized by different types:\n- numerical: Age (continuous, float64), Fare (continuous, float64), SibSp (discrete, int64), Parch (discrete, int64)\n- categorial: Sex (string), Pclass (int64), Embarked (character), Survived (int64), Ticket (alphanumeric, string), Cabin (alphanumeric, string), Name (string)\n"
    },
    {
      "metadata": {
        "_cell_guid": "f696b514-65b0-4c68-afc9-24d9c7ad5b76",
        "_uuid": "ad8140a6b4de1b76e866685fd9c615e3b0a1b22d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# read data and have a first look at it\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine = [train_df, test_df]\ntrain_df.info()\nprint('_'*40)\ntest_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c6a07f39-00ee-44e7-8c63-b8b1ab325425",
        "_uuid": "fe63088da87c3e8c058d98759343029bf687867c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# look at the first five rows\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4e0f38e6-35f2-46dc-9c34-d0c88ac0d311",
        "_uuid": "38c3f85679cf49944fd1bfc722d8088121dde7ae",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# look at the first five rows\ntest_df.head() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "463d0a51-b3a1-4727-9468-74c3f0e45758",
        "_uuid": "788e866d0c76a0a37c90b8beea35c2cbf4b0face",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# describe numerical data\ntrain_df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "956858f4-7cda-4c5e-be06-e9a45caa708e",
        "_uuid": "47a2deaa936890dfb617a31e7db56b5dc0c8e715",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# describe numerical data\ntest_df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "596013e4-944c-4c44-9536-3753fc029e7d",
        "_uuid": "48c9d13d71f64ce0280025b93540353ee703778d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# describe object data\ntrain_df.describe(include=['O'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "27528cc9-a2b9-4bc4-a391-e30b38425905",
        "scrolled": true,
        "_uuid": "9a01e53cae1125459c83d26367ad58f5bbd3038f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# describe object data\ntest_df.describe(include=['O'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8b52a7ab-7b4b-4074-9461-66c3c0e27e1d",
        "_uuid": "2818d95b9ce19d3f1fa55895fb99abaca87fca49",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# check Pclass - Survived correlation\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "073264d9-9ee9-430a-a789-d7a9ea8ce819",
        "_uuid": "a13fda47b1f19675626428444e0d654674c6a6a4",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# check Sex - Survived correlation\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f06e19f9-41c7-48f5-99af-3131ebc1bcb8",
        "_uuid": "493254854cddf43d35ae88ecccd39fdf2fc32d61",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# check SibSp - Survived correlation\ntrain_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f11b085e-12ab-4ae4-a30e-6f7fa84cdec1",
        "_uuid": "5b62f8794633932930bc14526dbe428ab9c4ed61",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# check Parch - Survived correlation\ntrain_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b9e3c595-52fc-4ce1-bca8-3d3c569d1155",
        "_uuid": "25ea5d350a8282b17fa20723bfd421495659c671",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Age histograms depending on Survived\ngrid = sns.FacetGrid(train_df, col='Survived');\ngrid.map(plt.hist, 'Age', bins=20);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9a71b19e-956e-4852-8b85-76aa31822268",
        "_uuid": "71efb8aa83cda4bb1a9e668d204fa4f180429433",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Age histograms depending on Survived, Pclass\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "537c62c3-1f43-4dbb-83a8-f7f0f0cf60d5",
        "_uuid": "489e7932e751bf8e946d3c1b8aab1e233785d4a9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Survived values depending on Embarked, Sex\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6);\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep');\ngrid.add_legend();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a83cdbab-e89e-4590-b449-170f3aef58e8",
        "_uuid": "da065d1b2edd2423f6dd6212848a9e200213cc79",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Fare depending on Embarked, Survived, Sex\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f5e6bedd-9ad3-4324-93d6-b6d8836c0c49",
        "_uuid": "bd727fbb9d8a43457f301c93cf0dc8fe815e7579"
      },
      "cell_type": "markdown",
      "source": "# 3. Manipulate Data\n\nBy having analyzed the data we will perform the following steps:\n\n- convert Pclass feature\n- create new feature: Title\n- delete features: Ticket, Cabin, Name, PassengerId\n- convert features: Sex\n- complete and convert feature: Age\n- create new features: IsAlone, Age*Class\n- complete and convert feature: Embarked \n- complete and convert feature: Fare\n- delete Pclass feature"
    },
    {
      "metadata": {
        "_cell_guid": "df26ba21-8045-4880-8462-cdb6ed66e8c6",
        "_uuid": "5540128b73af8a04a395422dbad1b544ac407b47"
      },
      "cell_type": "markdown",
      "source": "### Convert Pclass feature"
    },
    {
      "metadata": {
        "_cell_guid": "fd6b1c02-d4b1-483d-9b41-7b8dd9b6a24a",
        "_uuid": "ac7f58bdb6f59fd58e22344f159d083e5e24cfe6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n# use one-hot-encoding for Pclass \nfor dataset in combine:\n    dataset['Pclass 1'] = dataset['Pclass'].map({1: 1, 2: 0, 3: 0}).astype(int)\n    dataset['Pclass 2'] = dataset['Pclass'].map({1: 0, 2: 1, 3: 0}).astype(int)\n    dataset['Pclass 3'] = dataset['Pclass'].map({1: 0, 2: 0, 3: 1}).astype(int)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ede3c3c4-dc9f-4800-b439-50b73ab93ca5",
        "_uuid": "ff612dee502f55a7352ad74f1a1c1bc5e1ac3897"
      },
      "cell_type": "markdown",
      "source": "### Create new feature: Title"
    },
    {
      "metadata": {
        "_cell_guid": "3c6470f3-e3fe-46de-8482-1e1036eb99ab",
        "_uuid": "048ae6c19ad3a4dc573453f50d34bc1e26f58dee",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# extract title from Name and then create new feature: Title  \nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Survived'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1750ae63-8689-40fa-9a48-27439183ba45",
        "_uuid": "fe3c7c785aa7f6b5706345cd5cf3bc2825cb3ff6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pd.crosstab(test_df['Title'], train_df['Sex'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "833d73d4-3216-4554-9d02-5d2a9cf086bd",
        "collapsed": true,
        "_uuid": "1e393f43ebcfad09d2fa03a4fe178e0c907c18b9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# reduce the number of titles\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir',\n                                                 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9749ba5c-b5c1-4348-ab40-3f7a1c22f471",
        "_uuid": "9c63ec3ab45a2d43321ea698f10abaa12066e596",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# no missing titles\nprint(train_df.Title.isnull().sum())\nprint(test_df.Title.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2e1ce6af-40fa-4ef4-bc20-1a8f7a50c16d",
        "_uuid": "76e7f8bf912678e8948560608609c581b0c43367",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"# use one-hot-encoding for Title\nfor dataset in combine:\n    dataset['Title Mr'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 0, \"Mrs\": 0, \"Master\": 0, \"Rare\": 0}).astype(int)\n    dataset['Title Miss'] = dataset['Title'].map({\"Mr\": 0, \"Miss\": 1, \"Mrs\": 0, \"Master\": 0, \"Rare\": 0}).astype(int)\n    dataset['Title Mrs'] = dataset['Title'].map({\"Mr\": 0, \"Miss\": 0, \"Mrs\": 1, \"Master\": 0, \"Rare\": 0}).astype(int)\n    dataset['Title Master'] = dataset['Title'].map({\"Mr\": 0, \"Miss\": 0, \"Mrs\": 0, \"Master\": 1, \"Rare\": 0}).astype(int)\n    dataset['Title Rare'] = dataset['Title'].map({\"Mr\": 0, \"Miss\": 0, \"Mrs\": 0, \"Master\": 0, \"Rare\": 1}).astype(int)\n\n# drop Title\ntrain_df = train_df.drop(['Title'],axis=1)\ntest_df = test_df.drop(['Title'],axis=1)\ncombine = [train_df, test_df]\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7881c55a-b273-4ff3-b5ef-3067f68c7fde",
        "_uuid": "8516d5308879a9c23fee143315fec74747a7cc93",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# use ordinal values for Title \ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "02a9e9f6-c72d-49b8-9f03-407934ac5dda",
        "_uuid": "30c909298fc876c03b542b7eb36f26997aa3282f"
      },
      "cell_type": "markdown",
      "source": " ### Delete features: Ticket, Cabin, Name, PassengerId"
    },
    {
      "metadata": {
        "_cell_guid": "339881dc-a267-4553-b4df-5ac1c34d09de",
        "_uuid": "5b0b89370e7b7362db6acb6915eb6c9feae05d66",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# delete columns: Ticket, Cabin, Name, PassengerId\ntrain_df = train_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\nprint(\"train_df = \", train_df.shape)\nprint(\"test_df = \", test_df.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6915c616-8fbf-479c-9350-e966102c1ab1",
        "_uuid": "30c5a5ca5805e67eb96d87edae088b9a0501a67e"
      },
      "cell_type": "markdown",
      "source": " ### Convert features: Sex"
    },
    {
      "metadata": {
        "_cell_guid": "4bad8e4c-27f5-4ec7-b5f1-5fa9a51f4c0a",
        "_uuid": "152ea9309d211154ce6cc0a94c649f7e0d1cb506",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# convert variable 'Sex' into type int64\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dd33c23a-35a6-4614-b032-89e4ba55787d",
        "_uuid": "7e9cddf2d40b2eedc7ffa33d9e1d64b1f1300862"
      },
      "cell_type": "markdown",
      "source": "### Complete and convert feature: Age"
    },
    {
      "metadata": {
        "_cell_guid": "92666ab6-92e2-496c-983d-9809331c2199",
        "_uuid": "f1ba3ac193e6a7f7bf4ae3ed2633eee18a663d9b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# complete missing age entries by using information on Sex, Pclass\nguess_ages = np.zeros((2,3));\n\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \n                               (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n            guess_ages[i,j] = int(age_guess/0.5 + 0.5 ) * 0.5\n            #print(age_guess)\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & \n                        (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c41b12f4-62de-4ea4-b085-f1ea982b45fe",
        "_uuid": "f32868c32eeb97af1bdf95d6516cfbcb6b23e949",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create new feature AgeBand\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "80329730-e5f2-4e99-b975-bd4dfa108991",
        "_uuid": "1b7a96abfbd2d6a933883ddf870cd7d9f21818aa",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Replace Age with ordinals based on the bands in AgeBand\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "27b9268a-c1cf-4cba-93f8-59c2205718b7",
        "scrolled": true,
        "_uuid": "471a7f63edbe0b7be514fa5ce77c4038ad3bd7f6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# remove AgeBand\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0ac53154-9ff5-4c1c-b5f3-2c9b5d91fbdd",
        "_uuid": "ba8c386ddd84948b992f2b4adc69cc233c675d0a"
      },
      "cell_type": "markdown",
      "source": "### Create new features: IsAlone, Age*Class"
    },
    {
      "metadata": {
        "_cell_guid": "2d076f61-8c88-4509-bd19-828d536cf926",
        "_uuid": "064ba062be5d0cc303081b20e7e5b85c39b15894",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create new feature FamilySize\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8b10a488-878d-4460-a642-131568285796",
        "_uuid": "ef1d0f1c394bb86a81d13c0ac1d7bf8858fd2212",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create new feature IsAlone\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c32dd512-ae87-4a0c-a5e1-5fb0bc997eda",
        "_uuid": "b5317852a9d312efac586a7c7ef8898354d5e294",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# remove features: Parch, SibSp, FamilySize\n#train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n#test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntrain_df = train_df.drop(['Parch', 'SibSp'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c9eaa4c9-c3af-4241-b117-8a5ea69fd3f1",
        "_uuid": "d7e4de39b1ac2addbb1c6d8bce5e13ecc5188c87",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create new feature Age*Class\nfor dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\n#train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c44e4349-77e0-492c-8d8d-a0001c85bbfa",
        "_uuid": "489a58dd294ce81096511b194f7f75f970c83e60"
      },
      "cell_type": "markdown",
      "source": "### Complete and convert feature: Embarked "
    },
    {
      "metadata": {
        "_cell_guid": "11e0b1e1-6aee-41f8-abc8-251356b2b8d4",
        "_uuid": "76949fcf9d6817464e6ef02b1261a89817836dff",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# only 2/0 missing values in train/test data\nprint(train_df.Embarked.isnull().values.sum())\nprint(test_df.Embarked.isnull().values.sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "09ca0df6-2880-474c-bc23-19e94a766281",
        "_uuid": "d1c424c98e58b98e3582013d8fdfd3ed47f2d378",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# most frequent occurence of Embarked value\nfreq_port = train_df.Embarked.dropna().mode()[0]\nprint(freq_port);\n\n# replace na entries with most frequent value of Embarked\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a9c30506-1dce-4cee-bf30-d1549f7cbb35",
        "_uuid": "dc4c7180109de6c7c7dd316b94529396a181fa7b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n# use one-hot-encoding for Embarked \nfor dataset in combine:\n    dataset['Embarked S'] = dataset['Embarked'].map({'S': 1, 'C': 0, 'Q': 0}).astype(int)\n    dataset['Embarked C'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 0}).astype(int)\n    dataset['Embarked Q'] = dataset['Embarked'].map({'S': 0, 'C': 0, 'Q': 1}).astype(int)\n\n# drop Embarked\ntrain_df = train_df.drop(['Embarked'],axis=1)\ntest_df = test_df.drop(['Embarked'],axis=1)\ncombine = [train_df, test_df]\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "924e0324-b852-42b8-a934-a7e06bfb852c",
        "_uuid": "0f18ea6356be3e906ab5592cdce6e910932b8a85",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# use ordinal values for Embarked\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4d5a0773-53de-4c7e-ab7c-709e0ef16efd",
        "_uuid": "ef90aa54ac2e6b864bba561be4392a27d1c8051b"
      },
      "cell_type": "markdown",
      "source": "### Complete and convert feature: Fare"
    },
    {
      "metadata": {
        "_cell_guid": "cf1a009c-9061-47ca-9572-8bda140796ee",
        "_uuid": "22dc9b02e09d9bafd1e217ad247893935ce7700f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# only 0/1 missing values in train/test data\nprint(train_df.Fare.isnull().values.sum())\nprint(test_df.Fare.isnull().values.sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "335505b5-fe55-49d6-99df-a42703b6a00f",
        "_uuid": "961a2c418877d7c17280a90189a772476dd15315",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# complete feature Fare in test set\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7c18595a-fff1-4ad0-91e9-db32a018cedf",
        "_uuid": "b87bd5292ed86a448dcbd999e6b534cfced94cd5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# create feature FareBand\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 5)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "867dc2b7-1e73-49cc-a381-c9fca45195bb",
        "_uuid": "88240358fae71136666cb82ee2636971eeba9548",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e6d4340f-b368-4c2b-b84d-792178e7d03d",
        "_uuid": "a2b444fb5267852f76935e0e995a3ce7a05e0878",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# replace feature Fare by ordinals based on FareBand\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b4a1fed7-e70c-4bfd-b67b-3f3c2de4ba82",
        "_uuid": "edb746dff9b7d589409a05d0cd4c2a7e431114dc",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8927df78-ab3b-4c4f-b551-bdb6edc4fa7b",
        "_uuid": "9f0a4f032b8c83c0555c96602ef5fe06d9f2ce8d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "48ab3a62-6343-4c6e-9629-f5715db4bb1c",
        "_uuid": "3164712b1133b7809d3411fe07002154fe7dee91"
      },
      "cell_type": "markdown",
      "source": "### Drop feature Pclass"
    },
    {
      "metadata": {
        "_cell_guid": "8d089c55-da5b-423c-bfd2-dd99efa8991b",
        "_uuid": "dbc1b82aa187745827c2a4ddf5f3d4b410c6bb69",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\"\"\"\n# drop Pclass\ntrain_df = train_df.drop(['Pclass'],axis=1)\ntest_df = test_df.drop(['Pclass'],axis=1)\ncombine = [train_df, test_df]\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "20ffee26-bb39-4a33-8e22-c71a5c8f8427",
        "_uuid": "cc3e41c2e9b3bbdd2272991cf9489439a21aeac4"
      },
      "cell_type": "markdown",
      "source": "\n# 4. Model data\n- create training, validation, testing sets\n- supervised learning plus cassification limits the number of machine learning algorithms to:  \n    - Logistic Regression\n    - kNN (k-Nearest Neighbors)\n    - SVM (Support Vector Machine) with different kernels\n    - Gaussian Naive Bayes\n    - Decision Tree\n    - Random Forrest\n    - Deep Neural Network\n- train models and summarize the results"
    },
    {
      "metadata": {
        "_cell_guid": "f67936b8-5a1b-4654-80aa-005b09274f15",
        "scrolled": true,
        "_uuid": "1bd3e2715c87bcffa4a1f3a9efdc32a87e47f208",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## create training, validation, testing sets\n\n# read train, validation, test data\nvalid_set_size = int(train_df.shape[0] * valid_set_size_percentage/100.0)\n\n# train on all data\nif train_on_all_data:\n    train_set_size = train_df.shape[0]\nelse:\n    train_set_size = train_df.shape[0] - valid_set_size\n    \nX_train_val = train_df.drop(\"Survived\", axis=1).copy().values\nY_train_val = train_df[\"Survived\"].copy().values.reshape(-1)\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy().values\ntest_set_size = X_test.shape[0]\n\n# store used features\ndf_features = pd.DataFrame(train_df.drop(\"Survived\", axis=1).columns.delete(0))\ndf_features.columns = ['Feature']\nprint(df_features)\nprint('')\n\n# normalize train, validation, test data\nX_train_val_norm = (X_train_val)/(X_train_val.max() - X_train_val.min());\nX_test_norm = (X_test)/(X_test.max()-X_test.min());\n\n# split train and validation data\nperm_array = np.arange(X_train_val_norm.shape[0])\nnp.random.shuffle(perm_array)\nX_train_norm = X_train_val_norm[perm_array[:train_set_size]]\nY_train = Y_train_val[perm_array[:train_set_size]]\nX_valid_norm = X_train_val_norm[perm_array[-valid_set_size:]]\nY_valid = Y_train_val[perm_array[-valid_set_size:]]\n\n#X_train = train_df.drop(\"Survived\", axis=1)[0:train_set_size]\n#Y_train = train_df[\"Survived\"][0:train_set_size]\n#if valid_set_size > 0:\n#    X_valid = train_df.drop(\"Survived\", axis=1)[train_set_size:]\n#    Y_valid = train_df[\"Survived\"][train_set_size:]\n#else:\n#    X_valid = train_df.drop(\"Survived\", axis=1)[801:]\n#    Y_valid = train_df[\"Survived\"][801:]\n    \nprint('X_train_norm.shape = ', X_train_norm.shape)\nprint('Y_train.shape = ', Y_train.shape)\nprint('X_valid_norm.shape = ', X_valid_norm.shape)\nprint('Y_valid.shape = ', Y_valid.shape)\nprint('X_test_norm.shape = ', X_test_norm.shape)\n\n# function to shuffle randomly train and validation data\ndef shuffle_train_valid_data():\n    global X_train_val_norm, X_train_norm, Y_train, X_valid_norm, Y_valid\n    np.random.shuffle(perm_array)\n    X_train_norm = X_train_val_norm[perm_array[:train_set_size]]\n    Y_train = Y_train_val[perm_array[:train_set_size]]\n    X_valid_norm = X_train_val_norm[perm_array[-valid_set_size:]]\n    Y_valid = Y_train_val[perm_array[-valid_set_size:]]\n    return None ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "27f7f7f1-aed6-49e5-9f6c-498d64e40ca7",
        "_uuid": "05c78c65f10ed33df9d76032b21ec03867647ed9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## Logistic Regression as a benchmark model\n\nacc_log_train = 0\nacc_log_valid = 0\nlog_correlation = 0\nY_log_pred = np.zeros(X_test_norm.shape[0])\n\nfor i in range(cv_num):\n    \n    shuffle_train_valid_data()\n\n    logreg = sklearn.linear_model.LogisticRegression()\n    logreg.fit(X_train_norm, Y_train)\n    Y_log_pred += logreg.predict_proba(X_test_norm)[:,1]\n\n    acc_log_train += logreg.score(X_train_norm, Y_train)\n    acc_log_valid += logreg.score(X_valid_norm, Y_valid)\n    log_correlation += logreg.coef_[0];\n\nacc_log_train /= cv_num\nacc_log_valid /= cv_num\nlog_correlation /= cv_num\nY_log_pred /= cv_num\n\nprint('Logistic Regression: train/valid Acc = %.4f/%.4f'%(acc_log_train, acc_log_valid))\ndf_features[\"Correlation\"] = pd.Series(log_correlation)\ndf_features.sort_values(by='Correlation', ascending=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "25d15df0-e81a-4f3e-a6e1-0a9e6728bf6e",
        "scrolled": true,
        "_uuid": "e5d76b8e14e7d077a2f3759fd7af371188202d85",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## Further Machine Learning Algorithms\n\nacc_svc_rbf_train = acc_svc_rbf_valid = 0\nacc_svc_linear_train = acc_svc_linear_valid = 0\nacc_knn_train = acc_knn_valid = 0\nacc_gaussianNB_train = acc_gaussianNB_valid = 0\nacc_decision_tree_train = acc_decision_tree_valid = 0\nacc_random_forest_train = acc_random_forest_valid = 0\n\nY_pred_random_forest = np.zeros(X_test.shape[0])\nY_pred_decision_tree = np.zeros(X_test.shape[0])\nY_pred_gaussianNB = np.zeros(X_test.shape[0])\nY_pred_knn = np.zeros(X_test.shape[0])\nY_pred_svc_linear = np.zeros(X_test.shape[0])\nY_pred_svc_rbf = np.zeros(X_test.shape[0])\n\nfor i in range(cv_num):\n\n    shuffle_train_valid_data()\n\n    # support vector machine with rbf kernel\n    svc_rbf = sklearn.svm.SVC(kernel='rbf')\n    svc_rbf.fit(X_train_norm, Y_train)\n    Y_pred_svc_rbf += svc_rbf.predict(X_test_norm)\n    acc_svc_rbf_train += svc_rbf.score(X_train_norm, Y_train)\n    acc_svc_rbf_valid += svc_rbf.score(X_valid_norm, Y_valid)\n\n    # support vector machine with linear kernel\n    svc_linear = sklearn.svm.SVC(kernel='linear')\n    svc_linear.fit(X_train_norm, Y_train)\n    Y_pred_svc_linear += svc_linear.predict(X_test_norm)\n    acc_svc_linear_train += svc_linear.score(X_train_norm, Y_train)\n    acc_svc_linear_valid += svc_linear.score(X_valid_norm, Y_valid)\n\n    # k-Nearest-Neighbour Algorithm\n    knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 5)\n    knn.fit(X_train_norm, Y_train)\n    Y_pred_knn += knn.predict(X_test_norm)\n    acc_knn_train += knn.score(X_train_norm, Y_train)\n    acc_knn_valid += knn.score(X_valid_norm, Y_valid)\n\n    # Gaussian Naive Bayes\n    gaussianNB = sklearn.naive_bayes.GaussianNB()\n    gaussianNB.fit(X_train_norm, Y_train)\n    Y_pred_gaussianNB += gaussianNB.predict_proba(X_test_norm)[:,1]\n    acc_gaussianNB_train += gaussianNB.score(X_train_norm, Y_train)\n    acc_gaussianNB_valid += gaussianNB.score(X_valid_norm, Y_valid)\n\n    # Decision Tree\n    decision_tree = sklearn.tree.DecisionTreeClassifier()\n    decision_tree.fit(X_train_norm, Y_train)\n    Y_pred_decision_tree += decision_tree.predict_proba(X_test_norm)[:,1]\n    acc_decision_tree_train += decision_tree.score(X_train_norm, Y_train)\n    acc_decision_tree_valid += decision_tree.score(X_valid_norm, Y_valid)\n\n    # Random Forest\n    random_forest = sklearn.ensemble.RandomForestClassifier(n_estimators=10)\n    random_forest.fit(X_train_norm, Y_train)\n    Y_pred_random_forest += random_forest.predict_proba(X_test_norm)[:,1] # prob for 1\n    acc_random_forest_train += random_forest.score(X_train_norm, Y_train)\n    acc_random_forest_valid += random_forest.score(X_valid_norm, Y_valid)\n\nacc_svc_rbf_train /= cv_num\nacc_svc_rbf_valid /= cv_num\n\nacc_svc_linear_train /= cv_num\nacc_svc_linear_valid /= cv_num\n\nacc_knn_train /= cv_num\nacc_knn_valid /= cv_num\n\nacc_gaussianNB_train /= cv_num\nacc_gaussianNB_valid /= cv_num\n\nacc_decision_tree_train /= cv_num\nacc_decision_tree_valid /= cv_num\n\nacc_random_forest_train /= cv_num\nacc_random_forest_valid /= cv_num\n\nY_pred_random_forest /= float(cv_num)\nY_pred_decision_tree /= float(cv_num)\nY_pred_gaussianNB /= float(cv_num)\nY_pred_knn /= float(cv_num)\nY_pred_svc_linear /= float(cv_num)\nY_pred_svc_rbf /= float(cv_num)\n\n#print(Y_pred_random_forest)\n\nprint('SVC rbf kernel: train/valid Acc = %.4f/%.4f'%(acc_svc_rbf_train, acc_svc_rbf_valid))\nprint('SVC linear kernel: train/valid Acc = %.4f/%.4f'%(acc_svc_linear_train, acc_svc_linear_valid))\nprint('kNN: train/valid Acc = %.4f/%.4f'%(acc_knn_train, acc_knn_valid))\nprint('Gaussian Naive Bayes: train/valid Acc = %.4f/%.4f'%(acc_gaussianNB_train, acc_gaussianNB_valid))\nprint('Decision Tree: train/valid Acc = %.4f/%.4f'%(acc_decision_tree_train, acc_decision_tree_valid))\nprint('Random Forest: train/valid Acc = %.4f/%.4f'%(acc_random_forest_train, acc_random_forest_valid))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2873867e-926f-471e-8787-e4453679d4ac",
        "_uuid": "1ec800285f3b259c739b42da52c5c8ad48e3b12f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "## Deep Neural Network\n\nx_size = X_train_norm.shape[1]; # number of features\ny_size = 1; # binary variable\nn_n_fc1 = 256; # number of neurons of first layer\nn_n_fc2 = 128; # number of neurons of second layer\nn_n_fc3 = 64; # number of neurons of third layer\n\n# variables for input and output \nx_data = tf.placeholder('float', shape=[None, x_size])\ny_data = tf.placeholder('float', shape=[None, y_size])\n\n# 1.layer: fully connected\nW_fc1 = tf.Variable(tf.truncated_normal(shape = [x_size, n_n_fc1], stddev = 0.1))\nb_fc1 = tf.Variable(tf.constant(0.1, shape = [n_n_fc1]))  \nh_fc1 = tf.nn.relu(tf.matmul(x_data, W_fc1) + b_fc1)\n\n# dropout\ntf_keep_prob = tf.placeholder('float')\nh_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n\n# 2.layer: fully connected\nW_fc2 = tf.Variable(tf.truncated_normal(shape = [n_n_fc1, n_n_fc2], stddev = 0.1)) \nb_fc2 = tf.Variable(tf.constant(0.1, shape = [n_n_fc2]))  \nh_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) \n\n# dropout\nh_fc2_drop = tf.nn.dropout(h_fc2, tf_keep_prob)\n\n# 3.layer: fully connected\nW_fc3 = tf.Variable(tf.truncated_normal(shape = [n_n_fc2, n_n_fc3], stddev = 0.1)) \nb_fc3 = tf.Variable(tf.constant(0.1, shape = [n_n_fc3]))  \nh_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3) \n\n# dropout\nh_fc3_drop = tf.nn.dropout(h_fc3, tf_keep_prob)\n\n# 3.layer: fully connected\nW_fc4 = tf.Variable(tf.truncated_normal(shape = [n_n_fc3, y_size], stddev = 0.1)) \nb_fc4 = tf.Variable(tf.constant(0.1, shape = [y_size]))  \nz_pred = tf.matmul(h_fc3_drop, W_fc4) + b_fc4  \n\n# cost function\ncross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_data, logits=z_pred));\n\n# optimisation function\ntf_learn_rate = tf.placeholder(dtype='float', name=\"tf_learn_rate\")\ntrain_step = tf.train.AdamOptimizer(tf_learn_rate).minimize(cross_entropy)\n\n# evaluation\ny_pred = tf.cast(tf.nn.sigmoid(z_pred),dtype = tf.float32);\ny_pred_class = tf.cast(tf.greater(y_pred, 0.5),'float')\naccuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_class, y_data ), 'float'))\n\nkeep_prob = 0.5; # dropout regularization with keeping probability\nlearn_rate_range = [0.01,0.005,0.0025,0.001,0.001,0.001,0.00075,0.0005,0.00025,0.0001,\n                   0.0001,0.0001,0.0001];\nlearn_rate_step = 100;\n\nn_epoch = 500; # number of epochs\ntrain_loss, train_acc, valid_loss, valid_acc = 0,0,0,0\n\nacc_DNN_train = 0\nacc_DNN_valid = 0\nloss_DNN_train = 0\nloss_DNN_valid = 0\nY_pred_DNN = np.zeros(X_test.shape[0]).astype(np.float);\n\nfor j in range(cv_num):\n    \n    # start TensorFlow session and initialize global variables\n    sess = tf.InteractiveSession() \n    sess.run(tf.global_variables_initializer())  \n\n    shuffle_train_valid_data() # shuffle data\n    Y_train = Y_train.reshape(-1,1)\n    Y_valid = Y_valid.reshape(-1,1)\n    n_step = -1;\n\n    # training model\n    for i in range(0,n_epoch):\n\n        if i%learn_rate_step == 0:\n            n_step += 1;\n            learn_rate = learn_rate_range[n_step];\n            print('set learnrate = ', learn_rate)\n\n        sess.run(train_step, feed_dict={x_data: X_train_norm, y_data: Y_train, \n                                        tf_keep_prob: keep_prob, tf_learn_rate: learn_rate})\n\n        if i%20==0:\n            train_loss = sess.run(cross_entropy,feed_dict={x_data: X_train_norm, \n                                                           y_data: Y_train, \n                                                           tf_keep_prob: 1.0})\n\n            train_acc = accuracy.eval(feed_dict={x_data: X_train_norm, \n                                                 y_data: Y_train, \n                                                 tf_keep_prob: 1.0})    \n\n            valid_loss = sess.run(cross_entropy,feed_dict={x_data: X_valid_norm, \n                                                           y_data: Y_valid, \n                                                           tf_keep_prob: 1.0})\n\n            valid_acc = accuracy.eval(feed_dict={x_data: X_valid_norm, \n                                                 y_data: Y_valid, \n                                                 tf_keep_prob: 1.0})      \n\n            print('%d epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(i+1, \n                            train_loss, valid_loss, train_acc, valid_acc))\n\n    acc_DNN_train += train_acc\n    acc_DNN_valid += valid_acc\n    loss_DNN_train += train_loss\n    loss_DNN_valid += valid_loss\n    \n    # prediction for test set\n    Y_pred_DNN += y_pred.eval(feed_dict={x_data: X_test_norm, \n                                        tf_keep_prob: 1.0}).flatten()\n    \n    sess.close();\n    \nacc_DNN_train /= float(cv_num)\nacc_DNN_valid /= float(cv_num)\nloss_DNN_train /= float(cv_num)\nloss_DNN_valid /= float(cv_num)\nY_pred_DNN /= float(cv_num)\n\n# final loss and accuracy\nprint('')\nprint('final: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(loss_DNN_train, \n                                                                      loss_DNN_valid, \n                                                                      acc_DNN_train, \n                                                                      acc_DNN_valid))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2c24065a-4aed-4bb7-a7db-f14a0a56716c",
        "scrolled": true,
        "_uuid": "9476d3d481c3306eb4a71945801fbd734a15e77c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# model summary\nmodels = pd.DataFrame({\n    'Model': ['SVC with rbf kernel', 'kNN', 'Logistic Regression', \n              'Random Forest', 'Gaussian Naive Bayes', 'SVC with linear kernel', \n              'Decision Tree', 'Deep Neural Network'],\n    'Train Acc': [acc_svc_rbf_train, acc_knn_train, acc_log_train, \n                  acc_random_forest_train, acc_gaussianNB_train,\n                  acc_svc_linear_train, acc_decision_tree_train, acc_DNN_train],\n    'Valid Acc': [acc_svc_rbf_valid, acc_knn_valid, acc_log_valid, \n                  acc_random_forest_valid, acc_gaussianNB_valid, acc_svc_linear_valid, \n                  acc_decision_tree_valid, acc_DNN_valid]})\nmodels.sort_values(by='Valid Acc', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d1e28fc3-b0d4-4838-91b0-96e9ced185d8",
        "_uuid": "8f29dd6900b27b67f6efaa220fbf6db4b039b5de"
      },
      "cell_type": "markdown",
      "source": "# 5. Predict and submit test results\n- combine prediction of probabilities of different algorithms for the test set\n- draw classes from probabilities or use fixed cuts\n- submit test results"
    },
    {
      "metadata": {
        "_cell_guid": "c9960691-9eb2-411d-9c82-6d9b05d5c4b7",
        "collapsed": true,
        "_uuid": "6206defdbf5316fd5e050d6ed14e7dcb19bb9c96",
        "trusted": false
      },
      "cell_type": "code",
      "source": "## combined prediction\n#Y_pred_submit = (Y_pred_DNN + Y_pred_random_forest + Y_pred_decision_tree)/3.0\nY_pred_submit = Y_pred_DNN\n\n# fixed cut\nY_pred_class_submit = np.greater(Y_pred_submit,0.5).astype(np.int) \n\n# draw from probability distribution\n#Y_pred_class_submit = [np.random.binomial(1,x) for x in Y_pred_submit] ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a9812ec5-e121-469e-ae19-c90efecf2896",
        "collapsed": true,
        "_uuid": "e44f786a96daaf149f8744f163748831ba8f1576",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# submit the best results\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_class_submit\n    })\n\n#if not os.path.exists(os.path.dirname(os.getcwd())+'/output'): \n#    print('create directory ', os.path.dirname(os.getcwd())+'/output')\n#    os.makedirs(os.path.dirname(os.getcwd())+'/output')\n#submission.to_csv('../output/submission.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d4e36479-4084-4e58-aa57-6813ca201148",
        "collapsed": true,
        "_uuid": "b2948a1583a04172cc2d363d83930646b1444e07",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "name": "python",
      "version": "3.6.4",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}